=============================================================
           Modular toolkit for Data Processing (MDP)
=============================================================

        Authors: Pietro Berkes and Tiziano Zito
        Email:   {p.berkes,t.zito}@biologie.hu-berlin.de
        Homepage: http://mdp-toolkit.sourceforge.net
        Download: http://sourceforge.net/projects/mdp-toolkit
        Current release: 1.1.0
        License: LGPL v2.1 (see COPYING file)
        Date: Mon Jun 13 2005

=============================================================

Semi-automatically generated by links from
  http://mdp-toolkit.sourceforge.net/index.html .


Quick start

   Modular toolkit for Data Processing (MDP) is a Python library to perform
   data processing. Already implemented algorithms include: Principal
   Component Analysis (PCA), Independent Component Analysis (ICA), Slow
   Feature Analysis (SFA), and Growing Neural Gas (GNG). Read the full list
   of implemented algorithms.

   Using MDP is as easy as:

 >>> import mdp
 >>> # perform pca on some data x
 ...
 >>> y = mdp.pca(x)
 >>> # perform ica on some data x using single precision
 ...
 >>> y = mdp.fastica(x, typecode='f')

   MDP is of course much more than this: it allows to combine different
   algorithms and other data processing elements (nodes) into data processing
   sequences (flows). Moreover, it provides a framework that makes the
   implementation of new algorithms easy and intuitive.

   For more information you could:
     * Read the long description of MDP
     * Take a look at the new Tutorial (pdf 200 KB)
     * Read the FAQ
     * Sneak through the API

     ----------------------------------------------------------------------

Description

   Modular toolkit for Data Processing (MDP) is a Python library to implement
   data processing elements (nodes) and to combine them into data processing
   sequences (flows).

   A node is the basic unit in MDP, and it represents a data processing
   element, like for example a learning algorithm, a filter, a visualization
   step etc. Each node can have a training phase, during which the internal
   structures are learned from training data (e.g. the weights of a neural
   network are adapted or the covariance matrix is estimated) and an
   execution phase, where new data can be processed forwards (by processing
   the data through the node) or backwards (by applying the inverse of the
   transformation computed by the node if defined). MDP is designed to make
   the implementation of new algorithms easy and intuitive, for example by
   setting automatically input and output dimension and by casting the data
   to match the typecode (e.g. float or double precision) of the internal
   structures. Most of the nodes were designed to be applied to arbitrarily
   long sets of data: the internal structures can be updated successively by
   sending chunks of the input data (this is equivalent to online learning if
   the chunks consists of single observations, or to batch learning if the
   whole data is sent in a single chunk). Already implemented nodes include
   Principal Component Analysis (PCA), Independent Component Analysis (ICA),
   Slow Feature Analysis (SFA), and Growing Neural Gas Network.

   A flow consists in an acyclic graph of nodes (currently only node
   sequences are implemented). The data is sent to an input node and is
   successively processed by the following nodes on the graph. The general
   flow implementation automatizes the training, execution and inverse
   execution (if defined) of the whole graph. Crash recovery is optionally
   available: in case of failure, the current state of the flow is saved for
   later inspection. A subclass of the basic flow class allows user-supplied
   checkpoint functions to be executed at the end of each phase, for example
   to save the internal structures of a node for later analysis.

   MDP supports the most common numerical extensions to Python and the symeig
   package (a Python wrapper for the LAPACK functions to solve the standard
   and generalized eigenvalue problems for symmetric (hermitian) positive
   definite matrices). MDP also includes graph (a lightweight package to
   handle graphs).

   When used together with SciPy (the scientific Python library) and symeig,
   MDP gives to the scientific programmer the full power of well-known C and
   FORTRAN data processing libraries. MDP helps the programmer to exploit
   Python object oriented design with C and FORTRAN efficiency.

   MDP has been written for research in neuroscience, but it has been
   designed to be helpful in any context where trainable data processing
   algorithms are used. Its simplicity on the user side together with the
   reusability of the implemented nodes could make it also a valid
   educational tool.

     ----------------------------------------------------------------------

Installation

     * Requirements:
          * Python >= 2.3
          * one of the following Python numerical extensions: Numeric,
            Numarray, or SciPy.

       For optimal performance, we recommend to use SciPy with LAPACK and
       ATLAS libraries, and to install the symeig module.

       If you experience some problems installing these programs, try looking
       at this FAQ.
     * Download: Download MDP 1.1.0 at SourceForge
     * Installation:
       Unpack the archive file, enter the project directory and type:
       python setup.py install

       If you want to use MDP without installing it on the system Python
       path:
       python setup.py install --prefix=/some_dir_in_PYTHONPATH/

       On Windows, the installation of binary distribution is as easy as
       executing the installer and following the instructions.
     * Testing:
       If you have successfully installed MDP, you can test your installation
       in a Python shell as follows:

 >>> import mdp
 >>> mdp.test.test()

 >>> import graph
 >>> graph.test.test()

       You can also try the benchmark functions (only useful with SciPy):

 >>> import mdp
 >>> mdp.test.benchmark()

       Together with MDP some demo scripts are distributed that illustrate
       the basic and advanced use of the library. They can be found in the
       package installation path in the subdirectory demo.

       The demo scripts are thought to be a sort of interactive tutorial if
       you don't feel like reading the HTML tutorial. You should not directly
       execute them. Open a script with your favorite text editor, read the
       comments and try the commands on a Python shell.

     ----------------------------------------------------------------------

Mantainers

   MDP has been written by Pietro Berkes and Tiziano Zito at the Institute
   for Theoretical Biology of the Humboldt University, Berlin. For comments,
   patches, feature requests, support requests, and bug reports (if any) you
   can either use the SourceForge mailing list or write directly to us (maybe
   faster).

   If you want to contribute some code or a new algorithm, please do not
   hesitate to submit it!

     ----------------------------------------------------------------------

Node List

     * PCANode: implementation of the Principal Component Analysis (PCA)
       algorithm, a.k.a. discrete Karhunen-Loève transform.
       References: I.T. Jolliffe, Principal Component Analysis,
       Springer-Verlag (1986).
     * WhiteningNode : this node performs a whitening (sphering) of the input
       data. Output data has zero mean, unit variance and different variables
       are decorrelated.
       References: I.T. Jolliffe, Principal Component Analysis,
       Springer-Verlag (1986).
     * FastICANode : implementation of the FastICA algorithm for Independent
       Component Analysis (ICA).
       References: Aapo Hyvärinen Fast and Robust Fixed-Point Algorithms for
       Independent Component Analysis, IEEE Transactions on Neural Networks,
       10(3):626--634, 1999.
     * CuBICANode : implementation of the CuBICA algorithm for ICA.
       References: Blaschke, T. and Wiskott, L. (2003). CuBICA: Independent
       Component Analysis by Simultaneous Third- and Fourth-Order Cumulant
       Diagonalization. IEEE Transactions on Signal Processing, 52(5), pp.
       1250--1256.
     * SFANode: implementation of the Slow Feature Analysis (SFA) algorithm.
       References: Wiskott, L. and Sejnowski, T.J. (2002). Slow Feature
       Analysis: Unsupervised Learning of Invariances. Neural Computation,
       14(4):715-770.
     * GrowingNeuralGasNode : implementation of the Growing Neural Gas
       algorithm. See also the demo script neuralgas_demo.py .
       References: B. Fritzke (1995), A Growing Neural Gas Network Learns
       Topologies in G. Tesauro, D. S. Touretzky, and T. K. Leen (editors),
       Advances in Neural Information Processing Systems, 7:625-632, MIT
       Press.
     * PolynomialExpansionNode: this node expands its input into the space of
       polynomials of a given degree by computing all monomials in the input
       variables.
     * TimeFramesNode: this node computes an output signal formed by copies
       of delayed versions of the input signal. For a possible application,
       see the demo script logistic_demo.py
     * HitParadeNode: this analysis node stores a given number of local
       maxima and minima of the input signal, separated by a minimum gap in
       time.
     * EtaComputerNode: this analysis node computes the eta values (a measure
       of temporal variation) of the normalized training data.
       References: Wiskott, L. and Sejnowski, T.J. (2002). Slow Feature
       Analysis: Unsupervised Learning of Invariances. Neural Computation,
       14(4):715-770.
     * NoiseNode: This node injects noise into the input data. Based on the
       implementation of Mathias Franzius.

     ----------------------------------------------------------------------

   SourceForge.net Logo Valid HTML 4.01!  
